#!/usr/bin/env python3

__author__ = 'Michael Ciccotosto-Camp'
__version__ = ''

# -*- coding: utf-8 -*-
"""best_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tW8wuUtfZZ_IsqoteS-oXZ48oZnMMJ08
"""

import argparse
import csv
import itertools
import os
import socket
import sys
from pprint import pprint

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from scipy import optimize
from sklearn.metrics import (accuracy_score, average_precision_score, f1_score,
                             precision_recall_curve)
from torch.nn.modules.loss import MSELoss
from torch.utils.data import DataLoader

if sys.platform.startswith('win32'):
    PROJECT_DIR = os.path.join(
        'D:\\', '2020', 'S2', 'STAT_4402', 'ASSESSMENT', 'STAT4402_PROJECT_CODE')
elif sys.platform.startswith('linux'):
    PROJECT_DIR = os.path.join(
        '/', 'home', 's4430291', 'Courses', 'STAT4402', 'STAT4402_PROJECT_CODE')

torch.manual_seed(123)


def convert_bool_arg(arg_in):
    """
    Converts a boolean command line argument into a python boolean object.

    Parameters:
        arg_in:
            The given command line argument.

    Return:
        The interrupted boolean value of the command line argument.
    """

    false_options = ('f', 'false', '0', 'n', 'no', 'nan', 'none')

    if arg_in.lower() in false_options:
        return False

    return True


parser = argparse.ArgumentParser(
    description="Runs MLP with a single hyperparameter combination.")

parser.add_argument('--latent_dim', type=int, default=300,
                    help='The total number of latent dims (IR+MS).')
parser.add_argument('--MS_ratio', type=float, default=0.1,
                    help='What proportion of the latent dims should be attributed to MS.')
parser.add_argument('--auto_lr', type=float, default=0.0001,
                    help='Learning rate or auto encoder.')
parser.add_argument('--auto_epoch_num', type=int, default=100,
                    help='The number of epochs to use.')
parser.add_argument('--auto_batch_size', type=int, default=50,
                    help='The batch size to use.')

parser.add_argument('--lr', type=float, default=0.0001,
                    help='Learning rate.')
parser.add_argument('--epoch_num', type=int, default=20,
                    help='The number of epochs to use.')
parser.add_argument('--batch_size', type=int, default=150,
                    help='The batch size to use.')
parser.add_argument('--hidden_layer_2_bool', type=convert_bool_arg, default=False, const=True, nargs='?',
                    help='Use a second hidden layer.')
parser.add_argument('--unit_1_layers', type=int, default=400,
                    help='Number of neurons in the first hidden layer.')
parser.add_argument('--unit_2_layers', type=int, default=250,
                    help='Number of neurons in the second hidden layer.')
parser.add_argument('--unit_3_layers', type=int, default=250,
                    help='Number of neurons in the third hidden layer.')

args = parser.parse_args()

lr = 0.0005
epoch_num = 250
# epoch_num = 20
batch_size = 100
HiddenLayer2Bool = True
Unit1Layers = 400
Unit2Layers = 250
Unit3Layers = 0

latent_dim = 500
MS_ratio = 0.15
auto_lr = 0.0001
# auto_epoch_num = 250
auto_epoch_num = 10
auto_batch_size = 20

ms_latent_dim = round(MS_ratio * latent_dim)
ir_latent_dim = latent_dim - ms_latent_dim

print("\n\n")
print("--------------------------------")
print("PARAMETERS:")
print("lr:", lr)
print("epoch_num:", epoch_num)
print("batch_size:", batch_size)
print("HiddenLayer2Bool:", HiddenLayer2Bool)
print("Unit1Layers:", Unit1Layers)
print("Unit2Layers:", Unit2Layers)
print("Unit3Layers:", Unit3Layers)
print()
print("latent_dim:", latent_dim)
print("MS_ratio:", MS_ratio)
print("auto_lr:", auto_lr)
print("auto_epoch_num:", auto_epoch_num)
print("auto_batch_size:", auto_batch_size)
print("--------------------------------")
print("\n\n", flush=True)


def load_project_data(x_data_path: str = 'IR_MS_FUNCTIONAL_X.npy', y_data_path: str = 'IR_MS_FUNCTIONAL_y.npy', train_size: float = None, test_size: float = None):
    """
    Demonstrates an example of loading the data for the project. The
    features vectors for the samples are stored in IR_MS_FUNCTIONAL_X.npy
    while the correspong labels are stored in IR_MS_FUNCTIONAL_y.npy. Labels
    and feature vectors will share the same index. For example the label for
        IR_MS_FUNCTIONAL_X[1412]
    will be
        IR_MS_FUNCTIONAL_y[1412]

    The feature vectors have the form:

        [ ... ir data ... , ... ms data ... ]

    where each entry is a floating point value. The labels will take the form

        [ FUNC_GRP_1 , FUNC_GRP_2 , ... , FUNC_GRP_N ]

    where FUNC_GRP_i is a binary value indicating whether or not that 
    functional group is present.

    Parameters:
        x_data_path (str):
            A path string to a file containing the feature vectors. The
            feature values should be stored across columns while the
            samples should be stored across different rows. The file may
            either be a csv of numpy binary file.

        y_data_path (str):
            A path string to a file containing the label vectors. The
            label values should be stored across columns while the
            different samples should be stored across rows. The file may
            either be a csv of numpy binary file.

        train_size (float):
            The ratio of data to include in the training set. By default
            0.75 samples are dedicated to the training set. 

        test_size (float):
            The ratio of data to include in the testing set. By default
            0.25 samples are dedicated to the testing set. 

    Return:
        Returns the tuple
            x_train, x_test, y_train, y_test

        NOTE:
            If both train_size and test_size are specified then they will be
            treated as a ratio to split the data. For example if train_size = 3
            and test_size = 2 then 3 / 5 of the total data will be dedicated to
            the training set and 2 / 5 will be dedicated to the testing set.
    """
    if train_size is not None:
        if train_size < 0:
            raise ValueError(
                "test_size and train_size must be positive values.")

    if test_size is not None:
        if test_size < 0:
            raise ValueError(
                "test_size and train_size must be positive values.")

    if (train_size is None and test_size is not None):
        if test_size > 1:
            raise ValueError("test_size must be ratio if train_size is None")

    if (test_size is None and train_size is not None):
        if train_size > 1:
            raise ValueError("train_size must be ratio if test_size is None")

    if x_data_path.lower().endswith(".csv"):
        IR_MS_FUNCTIONAL_X = np.loadtxt(x_data_path, dtype=float)
    elif x_data_path.lower().endswith(".npy"):
        IR_MS_FUNCTIONAL_X = np.load(x_data_path)
    else:
        raise NotImplementedError(
            f"Don't know how to deal with file type of {x_data_path}")

    if y_data_path.lower().endswith(".csv"):
        IR_MS_FUNCTIONAL_y = np.loadtxt(y_data_path, dtype=int)
    elif y_data_path.lower().endswith(".npy"):
        IR_MS_FUNCTIONAL_y = np.load(y_data_path)
    else:
        raise NotImplementedError(
            f"Don't know how to deal with file type of {y_data_path}")

    train_ratio = None

    # Find the test and train ratios
    if train_size is None and test_size is None:
        # Use the default train ratio of 0.75
        train_ratio = 0.75
    elif train_size is not None and test_size is None:
        train_ratio = train_size
    elif train_size is None and test_size is not None:
        train_ratio = 1 - test_size
    else:
        total = train_size + test_size
        train_ratio = train_size / total

    samples, _ = IR_MS_FUNCTIONAL_X.shape

    # Compute the index at which the split the training set
    train_index = round(samples * train_ratio)

    return IR_MS_FUNCTIONAL_X[:train_index], IR_MS_FUNCTIONAL_X[train_index:], \
        IR_MS_FUNCTIONAL_y[:train_index], IR_MS_FUNCTIONAL_y[train_index:]


x_data_path = os.path.join(
    PROJECT_DIR, 'data', 'IR_MS_FUNCTIONAL_X_ALDE.npy')
y_data_path = os.path.join(
    PROJECT_DIR, 'data', 'IR_MS_FUNCTIONAL_y_ALDE.npy')

X, X_test, Y, Y_test = load_project_data(
    x_data_path, y_data_path, train_size=0.8)

# Split the training data into IR and MS groups
X_IR = X[:, :-500]
X_MS = X[:, -500:]

torch.manual_seed(123)


# Build a class for the neural network - for the ae_mlp
class Net2(nn.Module):

    global Unit1Layers
    global Unit2Layers

    def __init__(self):
        super(Net2, self).__init__()

        # layer 1
        self.hidden1 = nn.Sequential(nn.Linear(latent_dim, Unit1Layers),  # no. latent variables
                                     nn.ReLU())

        # layer 2
        self.hidden2 = nn.Sequential(nn.Linear(Unit1Layers, Unit2Layers),
                                     nn.ReLU())

        # output
        self.output = nn.Linear(Unit2Layers, Y.shape[1])

    def forward(self, x):
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.output(x)
        x = torch.sigmoid(x)
        return x


class Net3(nn.Module):

    global Unit1Layers
    global Unit2Layers
    global Unit3Layers

    def __init__(self):
        super(Net3, self).__init__()

        # layer 1
        self.hidden1 = nn.Sequential(nn.Linear(latent_dim, Unit1Layers),  # no. latent variables
                                     nn.ReLU())

        # layer 2
        self.hidden2 = nn.Sequential(nn.Linear(Unit1Layers, Unit2Layers),
                                     nn.ReLU())

        # layer 3
        self.hidden3 = nn.Sequential(nn.Linear(Unit2Layers, Unit3Layers),
                                     nn.ReLU())

        self.output = nn.Linear(Unit3Layers, Y.shape[1])

    def forward(self, x):
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.hidden3(x)
        x = self.output(x)
        x = torch.sigmoid(x)
        return x

# Define an auto-encoder for IR only data


class autoencode_IR(nn.Module):
    def __init__(self):
        super(autoencode_IR, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(X_IR.shape[1], ir_latent_dim), nn.ReLU())
        self.decoder = nn.Linear(ir_latent_dim, X_IR.shape[1])

    def forward(self, x):
        latent = self.encoder(x)
        decoded = self.decoder(latent)
        decoded = torch.sigmoid(decoded)
        return decoded, latent

# Define an auto-encoder for MS only data


class autoencode_MS(nn.Module):
    def __init__(self):
        super(autoencode_MS, self).__init__()

        self.encoder = nn.Sequential(
            nn.Linear(X_MS.shape[1], ms_latent_dim), nn.ReLU())
        self.decoder = nn.Linear(ms_latent_dim, X_MS.shape[1])

    def forward(self, x):
        latent = self.encoder(x)
        decoded = self.decoder(latent)
        decoded = torch.sigmoid(decoded)
        return decoded, latent


IR_save_path = os.path.join(PROJECT_DIR, 'data', 'IR_network_dict.pkl')
MS_save_path = os.path.join(PROJECT_DIR, 'data', 'MS_network_dict.pkl')

trained_autoencoder_IR = autoencode_IR()
# Load in the parameters dict from the pickled file
trained_autoencoder_IR.load_state_dict(torch.load(IR_save_path))
# Put the model in eval mode
trained_autoencoder_IR.eval()

trained_autoencoder_MS = autoencode_MS()
# Load in the parameters dict from the pickled file
trained_autoencoder_MS.load_state_dict(torch.load(MS_save_path))
# Put the model in eval mode
trained_autoencoder_MS.eval()

# make the X_IR and X_MS data into tensors
X_IR = torch.tensor(X_IR, dtype=torch.float)
X_MS = torch.tensor(X_MS, dtype=torch.float)

# then extract all the relevant latent variables
_, latentIR = trained_autoencoder_IR(X_IR)
_, latentMS = trained_autoencoder_MS(X_MS)

print(latentIR)
print(latentMS)
